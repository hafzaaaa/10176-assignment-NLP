{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EXu878Jd8Obf",
        "outputId": "9e62dc29-eca1-4f8d-cbdf-ac00fbcea678"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file using pandas\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLP dataset/mental_disorders_reddit.csv')\n",
        "\n",
        "# Now you can work with the data in the DataFrame 'df'\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fdc85fZ8iks",
        "outputId": "688010e3-f73e-4b89-ab5e-4bd7d5fbb333"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                 title  \\\n",
            "0  Life is so pointless without others   \n",
            "1                           Cold rage?   \n",
            "2                I donâ€™t know who I am   \n",
            "3              HELP! Opinions! Advice!   \n",
            "4                                 help   \n",
            "\n",
            "                                            selftext  created_utc  over_18  \\\n",
            "0  Does anyone else think the most important part...   1650356960    False   \n",
            "1  Hello fellow friends ðŸ˜„\\n\\nI'm on the BPD spect...   1650356660    False   \n",
            "2  My [F20] bf [M20] told me today (after I said ...   1650355379    False   \n",
            "3  Okay, Iâ€™m about to open up about many things I...   1650353430    False   \n",
            "4                                          [removed]   1650350907    False   \n",
            "\n",
            "  subreddit  \n",
            "0       BPD  \n",
            "1       BPD  \n",
            "2       BPD  \n",
            "3       BPD  \n",
            "4       BPD  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install and import necessary libraries\n",
        "!pip install nltk pandas\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "# Download required nltk data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load your dataset (replace with actual path)\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/NLP dataset/mental_disorders_reddit.csv\")\n",
        "\n",
        "# Initialize lemmatizer and stemmer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Function to get Part of Speech (POS) tags for accurate lemmatization\n",
        "def get_wordnet_pos(word):\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "# Text Preprocessing function (cleaning, lemmatizing, stemming)\n",
        "def preprocess_text(text):\n",
        "    # 1. Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # 2. Remove special characters, numbers, and punctuation\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    # 3. Remove extra whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    # 4. Tokenize text\n",
        "    words = word_tokenize(text)\n",
        "    # 5. Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    # 6a. Lemmatize each word with POS tag\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
        "    # 6b. Apply stemming\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    # Join processed words back into single strings\n",
        "    lemmatized_text = \" \".join(lemmatized_words)\n",
        "    stemmed_text = \" \".join(stemmed_words)\n",
        "    return lemmatized_text, stemmed_text\n",
        "\n",
        "# Apply preprocessing function to each row of your dataset\n",
        "data[['cleaned_lemmatized_text', 'cleaned_stemmed_text']] = data['text_column'].apply(\n",
        "    lambda x: pd.Series(preprocess_text(x))\n",
        ")\n",
        "\n",
        "# Save the cleaned, lemmatized, and stemmed data to a new CSV file\n",
        "data.to_csv(\"/content/drive/MyDrive/NLP dataset/processed_mental_disorders_reddit.csv\", index=False)\n",
        "\n",
        "# Display the first few rows of the processed data\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "RCC93ECwMTUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved Word2Vec model if needed\n",
        "w2v_model = Word2Vec.load(\"/content/drive/MyDrive/NLP dataset/word2vec_model.model\")\n",
        "\n",
        "# Example: Get the vector for a specific word\n",
        "word_vector = w2v_model.wv['example_word']  # Replace 'example_word' with a word in your corpus\n",
        "print(\"Vector for 'example_word':\", word_vector)\n",
        "\n",
        "# Example: Find similar words\n",
        "similar_words = w2v_model.wv.most_similar('example_word', topn=5)\n",
        "print(\"Words similar to 'example_word':\", similar_words)\n"
      ],
      "metadata": {
        "id": "eXNRzzYYNgnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install pandas scikit-learn nltk\n",
        "\n",
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK stopwords and tokenizers\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load dataset (replace with the actual file path and column name)\n",
        "data = pd.read_csv(\"/path/to/your/dataset.csv\")  # Use your dataset here\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    words = word_tokenize(text)  # Tokenize\n",
        "    stop_words = set(stopwords.words(\"english\"))  # Define stopwords\n",
        "    words = [word for word in words if word.isalpha() and word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['processed_text'] = data['text_column'].apply(preprocess_text)  # Replace 'text_column' with your text column name\n",
        "\n",
        "# Bag of Words Encoding\n",
        "vectorizer = CountVectorizer(max_features=1000)  # Limit to 1000 features\n",
        "X_bow = vectorizer.fit_transform(data['processed_text'])\n",
        "\n",
        "# Convert BoW to DataFrame\n",
        "bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "print(\"Bag of Words Encoding:\\n\", bow_df.head())\n",
        "\n",
        "# One-Hot Encoding\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "\n",
        "# Prepare data by extracting unique words from each document\n",
        "unique_words_per_doc = data['processed_text'].apply(lambda x: list(set(x.split())))\n",
        "X_onehot = onehot_encoder.fit_transform(unique_words_per_doc)\n",
        "\n",
        "# Convert One-Hot Encoding to DataFrame\n",
        "onehot_df = pd.DataFrame(X_onehot, columns=onehot_encoder.get_feature_names_out())\n",
        "print(\"One-Hot Encoding:\\n\", onehot_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "id": "pH5den88NqxT",
        "outputId": "16284120-8065-4d8d-8959-55e19a85d26b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/path/to/your/dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-7532a7d52322>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Load dataset (replace with the actual file path and column name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/path/to/your/dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Use your dataset here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Text preprocessing function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/path/to/your/dataset.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install nltk spacy\n",
        "!python -m spacy download en_core_web_sm  # Download the English model for spaCy\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import spacy\n",
        "\n",
        "# Download NLTK data for tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Step 3: Load and preprocess the dataset\n",
        "data = pd.read_csv(\"/path/to/your/dataset.csv\")  # Replace with your actual dataset path\n",
        "\n",
        "# Check the dataset\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "print(\"Sample data:\\n\", data.head())\n",
        "\n",
        "# Preprocessing function to tokenize text\n",
        "def preprocess_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Apply preprocessing\n",
        "data['tokenized_text'] = data['text_column'].apply(preprocess_text)  # Replace 'text_column' with your text column name\n",
        "\n",
        "# Step 4: POS tagging with NLTK\n",
        "def pos_tag_nltk(tokens):\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "# Apply NLTK POS tagging\n",
        "data['pos_tags_nltk'] = data['tokenized_text'].apply(pos_tag_nltk)\n",
        "\n",
        "# Display results from NLTK\n",
        "print(\"POS tags using NLTK:\\n\", data[['tokenized_text', 'pos_tags_nltk']].head())\n",
        "\n",
        "# Step 5: POS tagging with spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pos_tag_spacy(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.pos_) for token in doc]\n",
        "\n",
        "# Apply spaCy POS tagging\n",
        "data['pos_tags_spacy'] = data['text_column'].apply(pos_tag_spacy)  # Apply on the original text\n",
        "\n",
        "# Display results from spaCy\n",
        "print(\"POS tags using spaCy:\\n\", data[['text_column', 'pos_tags_spacy']].head())\n"
      ],
      "metadata": {
        "id": "j8-koL-7Pq4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install necessary libraries\n",
        "!pip install nltk spacy textblob\n",
        "!python -m spacy download en_core_web_sm  # Download the English model for spaCy\n",
        "\n",
        "# Step 2: Import libraries\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "\n",
        "# Download VADER lexicon\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Step 3: Load dataset\n",
        "# Replace with your actual dataset path and make sure it has a text column\n",
        "data = pd.read_csv(\"/path/to/your/dataset.csv\")  # Use your dataset here\n",
        "print(\"Dataset shape:\", data.shape)\n",
        "print(\"Sample data:\\n\", data.head())\n",
        "\n",
        "# Step 4: Sentiment Analysis with VADER\n",
        "# Initialize the VADER sentiment analyzer\n",
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function for VADER sentiment analysis\n",
        "def vader_sentiment(text):\n",
        "    return vader_analyzer.polarity_scores(text)\n",
        "\n",
        "# Apply VADER sentiment analysis\n",
        "data['vader_sentiment'] = data['text_column'].apply(vader_sentiment)  # Replace 'text_column' with your text column name\n",
        "data['vader_compound'] = data['vader_sentiment'].apply(lambda x: x['compound'])  # Extract the compound score\n",
        "\n",
        "# Display VADER sentiment results\n",
        "print(\"VADER Sentiment Analysis:\\n\", data[['text_column', 'vader_compound']].head())\n",
        "\n",
        "# Step 5: Sentiment Analysis with TextBlob\n",
        "# Function for TextBlob sentiment analysis\n",
        "def textblob_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity  # Returns the polarity score\n",
        "\n",
        "# Apply TextBlob sentiment analysis\n",
        "data['textblob_sentiment'] = data['text_column'].apply(textblob_sentiment)\n",
        "\n",
        "# Display TextBlob sentiment results\n",
        "print(\"TextBlob Sentiment Analysis:\\n\", data[['text_column', 'textblob_sentiment']].head())\n",
        "\n",
        "# Step 6: Named Entity Recognition (NER) with spaCy\n",
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function for Named Entity Recognition\n",
        "def named_entity_recognition(text):\n",
        "    doc = nlp(text)\n",
        "    return [(ent.text, ent.label_) for ent in doc.ents]  # Return list of entities with labels\n",
        "\n",
        "# Apply NER\n",
        "data['ner_results'] = data['text_column'].apply(named_entity_recognition)\n",
        "\n",
        "# Display NER results\n",
        "print(\"Named Entity Recognition Results:\\n\", data[['text_column', 'ner_results']].head())\n"
      ],
      "metadata": {
        "id": "-E_TjlWYQAGt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}